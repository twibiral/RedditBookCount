{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Book Preparation\n",
    "\n",
    "Processing the list of books to extract titles and authors. 'Normalize' them to make it easier to identify them in the reddit posts (to ignore all whitespaces and special characters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-10-27 20:17:27 +02:00)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from Functions import normalize_text\n",
    "\n",
    "NON_ALPHANUMERIC = re.compile(r'[^a-zA-Z0-9]+')\n",
    "OL_BOOK_DUMP = Path('./data/ol_dump_works_2022-09-30.txt')\n",
    "OL_RATINGS_DUMP = Path('./data/ol_dump_ratings_2022-09-30.txt')\n",
    "EXTRACTED_BOOKS_PATH = Path('./data/books.csv')\n",
    "MOST_FREQUENT_WORDS = Path('./data/most_common_words')\n",
    "FAUX_BOOK_TITLES = Path('./data/faux_book_titles')\n",
    "\n",
    "%load_ext autotime"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-10-27 20:17:27 +02:00)\n"
     ]
    }
   ],
   "source": [
    "mult_spaces = re.compile(r'\\s\\s+')\n",
    "def clean_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove newlines etc. from the titles. This is NOT equal to normalize_text(...)\n",
    "    :param title: A book title.\n",
    "    :return: Cleaned book title.\n",
    "    \"\"\"\n",
    "    return mult_spaces.sub(' ', title.replace('\\n', '').replace('\\r', '')).strip()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 21.7 s (started: 2022-10-27 20:17:27 +02:00)\n"
     ]
    }
   ],
   "source": [
    "with open(OL_BOOK_DUMP, encoding='utf-8') as book_list:\n",
    "    complete = book_list.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31 ms (started: 2022-10-27 20:17:48 +02:00)\n"
     ]
    }
   ],
   "source": [
    "with open(OL_RATINGS_DUMP, encoding='utf-8') as ratings_list:\n",
    "    ratings = ratings_list.readlines()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-10-27 20:17:48 +02:00)\n"
     ]
    }
   ],
   "source": [
    "with open(MOST_FREQUENT_WORDS, encoding='utf-8') as word_list:\n",
    "    most_common_words = {word.strip() for word in word_list.readlines()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-10-27 20:17:48 +02:00)\n"
     ]
    }
   ],
   "source": [
    "# This is a list that compiles words/phrases collected during the development process that are very common and\n",
    "# can't be classified as book title with certainty when found in a post.\n",
    "if FAUX_BOOK_TITLES.exists() and FAUX_BOOK_TITLES.is_file():\n",
    "    with open(FAUX_BOOK_TITLES, encoding='utf-8') as word_list:\n",
    "        faux_titles = {word.strip() for word in word_list.readlines()}\n",
    "\n",
    "most_common_words |= faux_titles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10,754 phrases and words that are too common and are ignored as titles (e.g. \"she\", \"and\", ...)\n",
      "time: 578 ms (started: 2022-10-27 20:17:49 +02:00)\n"
     ]
    }
   ],
   "source": [
    "print(f'Found {len(most_common_words):,} phrases and words that are too common and are ignored as titles (e.g. \"she\", \"and\", ...)')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140,288 books with at least one rating of 2 or more in open library.\n",
      "time: 125 ms (started: 2022-10-27 20:17:49 +02:00)\n"
     ]
    }
   ],
   "source": [
    "# Remove all books that were never rated or only rated badly\n",
    "def extract_rating(entry):\n",
    "    entry = entry.split(\"\\t\")\n",
    "    return entry[0], int(entry[2])\n",
    "\n",
    "min_rating = 2\n",
    "rated_books = {rating[0] for entry in ratings if (rating := extract_rating(entry)) and rating[1] > min_rating}\n",
    "print(f'Found {len(rated_books):,} books with at least one rating of {min_rating} or more in open library.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/26201547 [00:00<?, ?Titles/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a175065c9a9e49ce968776649c9357b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected the titles of 113,490 books. All other 26,088,057 books were discarded.\n",
      "time: 2min 11s (started: 2022-10-27 20:17:49 +02:00)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "title_set = set()\n",
    "\n",
    "# Remove all books that never got a rating on open library, have less than 6 or more than 100 characters in the title and not have an authors name as title.\n",
    "def process_entry(entry):\n",
    "    entry = entry.split('\\t')[4]\n",
    "    entry = json.loads(entry)\n",
    "    if 'title' in entry and 'key' in entry and entry['key'].strip() in rated_books:\n",
    "        title = clean_title(entry['title'])\n",
    "        normalized_title = normalize_text(title)\n",
    "\n",
    "        if title[:4].lower() == 'the ':     # Starts with 'The ', 'A ' or 'An '\n",
    "            normalized_title = normalized_title[3:]\n",
    "        elif title[:3].lower() == 'an ':\n",
    "            normalized_title = normalized_title[2:]\n",
    "        elif title[:2].lower() == 'a ':\n",
    "            normalized_title = normalized_title[1:]\n",
    "\n",
    "        if normalized_title and (4 < len(normalized_title) < 100 or (normalized_title.isnumeric() and 3 < len(normalized_title) < 20)) \\\n",
    "                and normalized_title not in most_common_words and normalized_title not in title_set:\n",
    "            title_set.add(normalized_title)\n",
    "            return title, normalized_title\n",
    "\n",
    "as_list = [x for entry in tqdm(complete, unit='Titles') if (x := process_entry(entry))]\n",
    "print(f'Collected the titles of {len(as_list):,} books. All other {len(complete) - len(as_list):,} books were discarded.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113,490 books indexed.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('006 and a Half', '006andahalf'),\n ('01-01-00', '010100'),\n ('05:58', '0558'),\n ('08 the Planet of the Tortoise Driver Little Prince',\n  '08theplanetofthetortoisedriverlittleprince'),\n ('10,000 ways to say I love you', '10000waystosayiloveyou')]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 63 ms (started: 2022-10-27 20:20:00 +02:00)\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(as_list):,} books indexed.')\n",
    "as_list.sort(key=lambda x: x[1])\n",
    "as_list[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32 ms (started: 2022-10-27 20:20:00 +02:00)\n"
     ]
    }
   ],
   "source": [
    "f = list(filter(lambda x: len(x[1]) > 15, as_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31 ms (started: 2022-10-27 20:20:01 +02:00)\n"
     ]
    }
   ],
   "source": [
    "books_df = pd.DataFrame(as_list, columns=['Title', 'Normalized'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 187 ms (started: 2022-10-27 20:20:01 +02:00)\n"
     ]
    }
   ],
   "source": [
    "books_df.to_csv(EXTRACTED_BOOKS_PATH, encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
